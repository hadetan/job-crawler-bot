# =============================================================================
# Job Crawler Bot - Environment Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# Google Custom Search API Configuration
# -----------------------------------------------------------------------------
# Get your API key from: https://console.developers.google.com/
GOOGLE_API_KEY=your_api_key_here

# Get your Search Engine ID from: https://programmablesearchengine.google.com/
GOOGLE_SEARCH_ENGINE_ID=your_search_engine_id_here

# -----------------------------------------------------------------------------
# Serp API Configuration
# -----------------------------------------------------------------------------
SERP_API_KEY=your_api_key_here

# Search query for finding job listings
# Supports multiple sites using OR operator:
# Example: "software engineer (site:boards.greenhouse.io OR site:jobs.lever.co OR site:careers.example.com)"
# Single site example: "software engineer site:boards.greenhouse.io"
SEARCH_QUERY="software engineer (site:boards.greenhouse.io OR site:jobs.lever.co)"

# -----------------------------------------------------------------------------
# Crawler Performance Settings
# -----------------------------------------------------------------------------
# Number of concurrent browser instances (recommended: 3-5)
# Higher values = faster but more resource intensive
CONCURRENCY=3

# Maximum number of search result pages to crawl per query
# Each page typically contains ~10 results
# Can be overridden per run with --pages CLI flag (e.g., --pages=30)
MAX_PAGES=5

# Run browser in headless mode (true recommended for production)
# Set to false for debugging to see browser actions
HEADLESS=true

# Page load timeout in milliseconds (default: 30000 = 30 seconds)
PAGE_TIMEOUT=30000

# User agent string for browser requests
USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36

# -----------------------------------------------------------------------------
# Retry Configuration
# -----------------------------------------------------------------------------
# Maximum number of retry attempts for failed requests
MAX_RETRIES=3

# Delay between retry attempts in milliseconds
RETRY_DELAY=2000

# Maximum number of retry attempts for failed Google search pages (Stage 1 checkpoint)
# After reaching this limit, the page will be skipped
MAX_RETRY_COUNT=3

# -----------------------------------------------------------------------------
# Output Configuration
# -----------------------------------------------------------------------------
# Directory where crawled job data will be saved
# Structure: OUTPUT_DIR/stage1/, OUTPUT_DIR/stage2/, OUTPUT_DIR/stage3/
OUTPUT_DIR=./output

# -----------------------------------------------------------------------------
# Stage 2 Configuration - Job Link Extraction
# -----------------------------------------------------------------------------
# CSS selectors for finding job links on listing pages
# Comma-separated list of selectors to try
JOB_LINK_SELECTORS=a[href*="/jobs/"],a[href*="/job/"],a[href*="/careers/"]

# -----------------------------------------------------------------------------
# Stage 3 Configuration - Job Detail Extraction
# -----------------------------------------------------------------------------
# Note: Stage 3 uses intelligent content extraction with two layers:
# 1. Structured Data Extraction (JSON-LD Schema.org JobPosting)
# 2. Intelligent DOM Analysis (fallback with smart content detection)
#
# No manual selector configuration needed - the system automatically:
# - Detects and validates job content
# - Handles HTML entities and tags
# - Filters out navigation/footer elements
# - Extracts title, description, location, skills, and more
# - Validates content quality before saving
